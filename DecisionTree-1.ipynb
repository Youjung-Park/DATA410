{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods: Tree Bagging; Random Forests; Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked a bit in passing about a few ensemble methods when we talked about trees etc. Let's take some time to use them! We'll go over both the sklearn implementations, and try implementing both ourselves. In the 'do it yourself' part, I'll give you a single iteration, it is your job to put it together ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbwjd\\AppData\\Local\\Temp\\ipykernel_16828\\3203377957.py:14: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import Image, display\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from six import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll use some simulated data: concentric spheres of classes, see plots and examples here:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=4000, n_features=10,\n",
    "                                 n_classes=2, random_state=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "train_data, train_labels = X[:2000], Y[:2000]\n",
    "test_data, test_labels = X[2000:], Y[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what sklearn has in terms of ensemble methods. There are two interesting ones we can use right now, adaboost and random forests. We'll start by using the sklearn ones, then try implementing random forests ourselves!\n",
    "\n",
    "Be sure to reference the documentation at:  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Let's start with just executing some sklearn functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (a decision tree): 0.759\n",
      "Accuracy (a random forest): 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dbwjd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (adaboost with decision trees): 0.824\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(train_data, train_labels)\n",
    "\n",
    "print ('Accuracy (a decision tree):', dt.score(test_data, test_labels))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "print ('Accuracy (a random forest):', rfc.score(test_data, test_labels))\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "abc.fit(train_data, train_labels)\n",
    "print ('Accuracy (adaboost with decision trees):', abc.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like ensemble methods do well, both do better than a single tree. Before moving on, try playing arond with some of the parameters, such as:\n",
    "\n",
    "n_estimators in RandomForestClassifier\n",
    "\n",
    "\n",
    "n_estimators and learning_rate AdaBoostClassifier\n",
    "\n",
    "Why do the methods behave as they when you tweak the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we consider the more widely usedRandom forests, which are combinations of many decision trees, let's start with a slightly simplified version: **tree bagging**. Here is a simple algorithm for tree bagging:\n",
    "\n",
    "1. Set B (number of trees to make)\n",
    "2. Repeat B times:\n",
    "  1. Draw N random samples from training data, with replacement, where N is the number of training data points\n",
    "  2. Fit a decision tree to this re-sampled data\n",
    "  3. Store the predictions from this decision tree on the test data\n",
    "3. As the final predictions on the test data, use the majority vote classification for the predictions above\n",
    "\n",
    "Below, I've given you an implementation of a single iteration of the main loop above. Complete the algorthim by (1) adding the repeated B resampling and fitting (2) implementing step 3 above, the final predictions from tree bagging.\n",
    "\n",
    "Once you've done that, does bagging do better than a single tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Tree Bagging): 0.8815\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "B = 500\n",
    "n = train_data.shape[0]\n",
    "sn = int(n*2.0/3.0)   # nr of training data in subset for each tree\n",
    "nf = train_data.shape[1]\n",
    "all_preds = np.zeros((B,test_data.shape[0]))\n",
    "\n",
    "for b in range(B):\n",
    "    bs_sample_index = np.random.choice(range(n), size=sn, replace=True)\n",
    "\n",
    "    ##   YOUR CODE HERE\n",
    "    dt.fit(train_data[bs_sample_index], train_labels[bs_sample_index])\n",
    "    all_preds[b][:] = dt.predict(test_data)\n",
    "    \n",
    "    \n",
    "voting = np.sum(all_preds,axis=0) / B\n",
    "voting = [int(x >= 0.5) for x in voting]\n",
    "b_accuracy = np.mean(voting==test_labels)\n",
    "print ('Accuracy (Tree Bagging):', b_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree bagging produces better accuracy than a single tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to do **random forests**. Random forests add the twist of subsampling features at each node. Typically, we take p' = sqrt(p) features. DecisionTreeClassifier implements with through the *max_features*, check out the documentation. A simple change to your above code should give you random forests.\n",
    "\n",
    "1. Set B (number of trees to make)\n",
    "2. Repeat B times:\n",
    "  1. Draw N random samples from training data, with replacement, where N is the number of training data points\n",
    "  2. Draw p' = sqrt(p) features without replacement\n",
    "  3. Fit a decision tree to this re-sampled data\n",
    "  4. Store the predictions from this decision tree on the test data\n",
    "3. As the final predictions on the test data, use the majority vote classification for the predictions above\n",
    "\n",
    "Does random forests do better than tree bagging?\n",
    "\n",
    "Note: you can also use trees, tree bagging, and random forests for regression! Now, the original data is a regression problem so just reload the data, and to do all of these ideas using trees, you need only use DecisionTreeRegressor instead of DecisionTreeClassifier; see:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "As a bonus, try implementing trees, tree bagging, and random forests for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Random Forest): 0.8625\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "B = 500\n",
    "n = train_data.shape[0]\n",
    "sn = int(n*2.0/3.0)   # nr of training data in subset for each tree\n",
    "nf = train_data.shape[1]\n",
    "all_preds = np.zeros((B,test_data.shape[0]))\n",
    "\n",
    "for b in range(B):\n",
    "    bs_sample_index = np.random.choice(range(n), size=sn, replace=True)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    nf_sample_index = np.random.choice(range(nf), size=round(nf**0.5), replace=True)\n",
    "\n",
    "    dt.fit(train_data[bs_sample_index][:, nf_sample_index], train_labels[bs_sample_index])\n",
    "    all_preds[b][:] = dt.predict(test_data[:, nf_sample_index])    \n",
    "    \n",
    "    \n",
    "voting = np.sum(all_preds,axis=0) / B\n",
    "voting = [int(x >= 0.5) for x in voting]\n",
    "rf_accuracy = np.mean(voting==test_labels)\n",
    "print ('Accuracy (Random Forest):', rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest produces a slightly lower accuracy than tree bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemneting models using DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "dtrg = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decisiotn tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Decision Tree): 0.261\n"
     ]
    }
   ],
   "source": [
    "dtrg.fit(train_data, train_labels)\n",
    "\n",
    "pred = dtrg.predict(test_data)\n",
    "\n",
    "eval_mae = mean_absolute_error(test_labels, pred)\n",
    "print('Mean Absolute Error (Decision Tree):', eval_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Tree Bagging): 0.29013300000000003\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "B = 500\n",
    "n = train_data.shape[0]\n",
    "sn = int(n*2.0/3.0)   # nr of training data in subset for each tree\n",
    "nf = train_data.shape[1]\n",
    "all_preds = np.zeros((B,test_data.shape[0]))\n",
    "\n",
    "for b in range(B):\n",
    "    bs_sample_index = np.random.choice(range(n), size=sn, replace=True)\n",
    "\n",
    "    ##   YOUR CODE HERE\n",
    "    dtrg.fit(train_data[bs_sample_index], train_labels[bs_sample_index])\n",
    "    all_preds[b][:] = dtrg.predict(test_data)\n",
    "    \n",
    "\n",
    "average = np.sum(all_preds,axis=0) / B\n",
    "\n",
    "eval_mae = mean_absolute_error(test_labels, average)\n",
    "print('Mean Absolute Error (Tree Bagging):', eval_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (Random Forest): 0.419626\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "B = 500\n",
    "n = train_data.shape[0]\n",
    "sn = int(n*2.0/3.0)   # nr of training data in subset for each tree\n",
    "nf = train_data.shape[1]\n",
    "all_preds = np.zeros((B,test_data.shape[0]))\n",
    "\n",
    "for b in range(B):\n",
    "    bs_sample_index = np.random.choice(range(n), size=sn, replace=True)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    nf_sample_index = np.random.choice(range(nf), size=round(nf**0.5), replace=True)\n",
    "\n",
    "    dtrg.fit(train_data[bs_sample_index][:, nf_sample_index], train_labels[bs_sample_index])\n",
    "    all_preds[b][:] = dtrg.predict(test_data[:, nf_sample_index])    \n",
    "    \n",
    "    \n",
    "average = np.sum(all_preds,axis=0) / B\n",
    "\n",
    "eval_mae = mean_absolute_error(test_labels, average)\n",
    "print('Mean Absolute Error (Random Forest):', eval_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DecisionTree-1.ipynb to html\n",
      "[NbConvertApp] Writing 313583 bytes to DecisionTree-1.html\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to html DecisionTree-1.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
